# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/dk19934/733-data-mining/blob/HOMEWORK-2/Untitled0.ipynb
"""

pip install ydata-profiling

import numpy as np
import pandas as pd
from ydata_profiling import ProfileReport
df=pd.read_csv('red_wine.csv')
df=pd.read_csv('red_wine.csv')
profile = ProfileReport(df, title="Profiling Report")
profile.to_widgets()
profile.to_notebook_iframe()
profile.to_file("assignment1.html")

import pandas as pd
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.dummy import DummyClassifier
from sklearn.metrics import roc_auc_score, accuracy_score
from sklearn.model_selection import cross_val_score, StratifiedKFold

# Load the red wine dataset from a CSV file
df = pd.read_csv("red_wine.csv")

# Separate the features and target variable
X = df.iloc[:, :-1]
y = df.iloc[:, -1]

# Define the models
nb = GaussianNB()
svm = SVC(kernel='linear', probability=True)
dt = DecisionTreeClassifier()
rf = RandomForestClassifier(n_estimators=100)
lr = LogisticRegression(max_iter=1000)

# Define the baseline models
oner = DummyClassifier(strategy='most_frequent')
zeror = DummyClassifier(strategy='constant', constant='low')

# Define the cross-validation strategy
cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

# Define the performance metrics
metrics = {'AUC': roc_auc_score, 'Accuracy': accuracy_score}

# Fit and evaluate the models using 10-fold cross-validation
results = []

for model in [nb, svm, dt, rf, lr, oner, zeror]:
    row = {'Model': type(model).__name__}

    
    for metric_name, metric_func in metrics.items():
        if metric_name == 'AUC':
            scores = cross_val_score(model, X, y, cv=cv, scoring='roc_auc', error_score='raise')
        else:
            scores = cross_val_score(model, X, y, cv=cv, scoring=metric_name.lower(), error_score='raise')
        
        mean_score = scores.mean()
        std_dev = scores.std() * 2
        
        row[metric_name] = mean_score
        row[metric_name + ' (std)'] = std_dev
    
    results.append(row)
    
# Assemble the results into a Pandas DataFrame
df_results = pd.DataFrame(results)

# Set the Model column as the index
df_results.set_index('Model', inplace=True)

# Display the results
print(df_results)

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_curve, auc
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt

# Load the red wine dataset from a CSV file
df = pd.read_csv("red_wine.csv")

# Separate the features and target variable
X = df.iloc[:, :-1]
y = df.iloc[:, -1]

# Convert the target variable to binary labels
le = LabelEncoder()
y = le.fit_transform(y)

# Split the dataset into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train a Random Forest classifier on the training set
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Predict the probabilities of the positive class for the test set
y_score = rf.predict_proba(X_test)[:, 1]

# Compute the false positive rate, true positive rate and thresholds for the ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_score)

# Compute the area under the ROC curve
roc_auc = auc(fpr, tpr)

# Plot the ROC curve
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic example')
plt.legend(loc="lower right")
plt.show()

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report

# Load the white wine dataset from a CSV file
df = pd.read_csv("white_wine.csv")

# Separate the features and target variable
X = df.iloc[:, :-1]
y = df.iloc[:, -1]

# Train a decision tree classifier on the dataset
dt = DecisionTreeClassifier(random_state=42)
dt.fit(X, y)

# Predict the target variable for the entire dataset
y_pred = dt.predict(X)

# Print a classification report for the model performance
print(classification_report(y, y_pred))

pip install pydotplus

from sklearn.tree import export_graphviz
import pydotplus
from IPython.display import Image
from sklearn.tree import DecisionTreeClassifier, export_graphviz
import pydotplus
from IPython.display import Image

# Load the white wine dataset
df = pd.read_csv("white_wine.csv")

# Separate the features and target variable
X = df.iloc[:, :-1]
y = df.iloc[:, -1]

# Train a decision tree classifier on the dataset
clf = DecisionTreeClassifier(max_depth=3)
clf.fit(X, y)

# Export the decision tree to a Graphviz file
dot_data = export_graphviz(clf, out_file=None, feature_names=X.columns, class_names=['0', '1'], filled=True, rounded=True, special_characters=True)

# Convert the Graphviz file to an image and display it
graph = pydotplus.graph_from_dot_data(dot_data)
Image(graph.create_png())